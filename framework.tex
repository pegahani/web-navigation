
%%%%%%%%
\begin{figure}[!t]
\centering
\includegraphics[scale=0.3]{./images/neural-network.png}
\caption{Deep Q-network framework for our web navigation system \PA{In Ivan's new experiments he proposed to replace the neural network with LSTM, while the input state is a new snippet.} \question{Since LSTM is for sequential learning approaches which part of our learning system is sequential?}}
\label{fig:naviagte}
\end{figure}
%%%%%%%%%

In aim of extracting accurate name entities (containing organisation names and dates) and tracking experts we propose a Markov Decision Process (MDP) model and list of query types for searching the web using various search engines. Query types and the MDP model are explained in the following.

\paragraph{Queries: }  %Since \textit{unoporuno} is an intelligent web navigation system, it requires some search engines and query generation models.
For generating snippets and extracting information from them, we use five search engines including DuckDuckGo, Google, Bing, CiteSeerx and Researchgate .As the person name comes from an available database (supported by the sociologists), we generate $7$ possible number of queries for each name in each search engine: \\

$<$person name$>$ $+$ (  [] $|$ doctorate $|$ institute $|$ master $|$ undergraduate $|$ university ) 

\paragraph{Markov Decision Process: } Using a similar framework as Narasimhan et al. \shortcite{narasimhan2016improving}, we model our web navigator model as a Markov Decision Process (MDP) \cite{puterman1994}. The MDP parameters including States, actions and rewards are defined as below:

\begin{itemize}
	\item %The \textit{states} should be modeled in a way indicating the difference between the current and new snippets. 
	 We utilise two models of \textit{sates} in this work regarding the Deep-Q network method and its contained neural network for computing the Q-value functions:
	\begin{itemize}
		\item \textit{Vector based}: each snippet is indicated with a vector of real values containing: \\
		-- one-hot $7$ encoding indicating which query type is used for generating the snippet,\\
	    -- one-hot $4$ dimensional encoding indicating which research engine is used, \\
		-- number of common organisation names and dates between gold standards for a given person and extracted name entities in the new snippet,\\
        -- confident score for extracted organisation names and dates. 
		\item \textit{text based}: Each snippet is considered as a state.
	\end{itemize}
	\item We have two \textit{Actions} including $q$ and $d$ indicating changing query and staying on the current query respectively. 
	\item The system goal is to extract the closest extracted organisation names and dates to the gold standards by generating less number of queries. For this reason, choosing the $q$ action in each state has a negative reward. And every time the extracted organisation name or date from a new snippet is close to the gold standards, the system receives a positive reward, for instance $r(s, q) = -0.1$ or $r(s, d) = 10$ if the extracted organisation name (year) is in the list of organisation names (years) for the gold standards. 
\end{itemize}

%%%%%%%%%%%%%
\begin{algorithm}
 \KwData{Initialize an experience replay memory $\mathcal{D}$ with $N$ capacity (\PA{It should be explained in experiments})}
 Initialize a Q-value function $Q$ with random weight $\theta$\;
 \KwResult{$Q$-value function after training on snippets}
 \For{episode=0 \KwTo $M$ }{
 	get a random personid and initialise environment with start state $s_1$
 }
 \For{$t=1$ \KwTo $T$}{
	\eIf{random() $<\epsilon$}{ 
 	select a random action $a_t \in A$
 	}{$a_t = \text{argmax}_a Q(s_t, a; \theta)$}
 }
Execute action $a_t$ and obtain reward $r_t$ and new state $s_{t+1}$\\
$\mathcal{D} \longleftarrow (s_t, a_t, r_t, s_{t+1}) \cup \mathcal{D}$\\
sample eandom mini-batch of transitions $(s_j, a_j, r_j, s_{j+1}) $ from $\mathcal{D}$\\
\eIf{$s_{j+1}$ is terminal}{
	$y_j = r_j$
}{
	$y_j = r_j + \gamma \text{max}_{a'} Q(s_{j+1}, a';\theta)$ (\PA{this part should be explained and modified. Because we test double Q network too in the experiments.})
}
perform gradient descent on the loss $L(\theta)$ based on $y_j$ (\PA{that should be explained too})\\
\If{$s_{t+1}$ is a terminal state}{
\textbf{break}}
\BlankLine
 \caption{Training Procedure via Deep Q- Network (\question{may be it is not necessary to write its pseudo code again.})}
\end{algorithm}	
%%%%%%%%%%%%%