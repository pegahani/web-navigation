In the Reinforcement Learning environment with a state set $S = s_1, \cdots, s_m$ and a  set of possible actions $A = a_1, \cdots, a_m$, the agent interacts with the environment by selecting an action $a$ in each state $s$ and going to the next state $s'$ until the terminal state condition is satisfied. This process is guided by a function namely policy $\pi : S \longrightarrow A$ by interacting the environment. The policy is calculable by assigning a punishment or reward $r(s, a)$ to each selected action $a$ in any state $s$.  

A way of defining the quality of policies is the Q-value function $Q : S \times A \longrightarrow R$ given by: 
$$Q^{\pi}(s, a) = \mathbb{E}[R_1+ \gamma R_2 + \cdots | s_0 = s, a =a, \pi]$$

which is the expectation of sum of rewards by starting in state $s$ taking action $a$ and following policy $\pi$. Here $\gamma \in (0, 1]$ is a discount factor for future rewards. If Q-value function is computable for all possible policies in the system the optimal policy should be: $\forall  s \in S \; \pi^*(s) = \text{argmax}_a Q^{\pi^*}(s, a) \; $.  

Q-learning algorithm \cite{sutton1998} is a model free method for learning the optimal Q-value function. Since Q-value function should be calculated for each sequence of state$\backslash$actions, computing $Q(s, a)$ for real environments with great number of states or actions  is impractical.  To tackle this problem Mnih et al.\shortcite{mnih2015} introduce the Deep Q-network for approximating the Q-value function with a non-linear multi layer convolutional neural network.  Given state $s$ and action $a$, the DQN gives $Q(s, a; \theta)$ value where $\theta$ are parameters for the neural network. \question{If we model the Q-value function as $Q(s, .; \theta)$ where gets sate $s$ and outputs a vector of action values is better in term of calculation time. Our current model is the naive form.} 

\paragraph{Queries: }  Since \textit{unoporuno} is an intelligent web navigation system, it requires some search engines and query generation models. In our work , we use four search engines including Google, DuckDuckGo, Researchgate, Citeceerx \PA{the search engine list should be modified. I didn't find the list.} . As person name is received from an available sociologist database, for each person we have $7$ possible number of queries as: \\

$<$person name$>$ $+$ (  $|$ doctorate $|$ institute $|$ master $|$ undergraduate $|$ university ) 

\paragraph{Markov Decision Process: } Using a similar framework as Narasimhan et al. \shortcite{narasimhan2016improving}, we model our web navigator model as a Markov Decision Process (MDP) \cite{puterman1994}. . .

This MDP is defined as a tuple $M(S, A, P, r, \gamma)$ where $S$ is a set of states, $A$ is a set of actions, $P :S\times A  \times S  \longrightarrow [0,1]$ is a transition function where $P(s'|s,a)$ encodes the probability of going to state $s'$ by being in state $s$, and choosing action $a$; $r : S \times A \longrightarrow R$ is a reward function (or penalty, if negative) obtained by choosing action $a$ in state $s$. Each parameter is detailed as the following.
\begin{itemize}
	\item The \textit{states} should be modeled in a way indicating the difference between the current and new snippets.  We use two models of sates in this paper (\question{I am wondering that there is no comparison between current and previous snippets in this model. I got the model from our code!}):
	\begin{itemize}
		\item \textit{Vector based}: each state indicates with a vector of real values containing: \\
		-- one-hot $7$ encoding indicating which query type is used for generating the snippet,\\
	    -- one-hot $4$ dimensional encoding indicating which research engine is used, \\
		-- number of common organisation names and dates between gold standards for a given person and extracted name entities in the new snippet,\\
        -- confident score for extracted organisation names and dates. 
		\item \textit{text based}: Each snippet is considered as a state ( \PA{for keeping this definition we should be certain about Ivan codes and results.} )
	\end{itemize}
	\item We have two \textit{Actions} including $q$ and $d$ indicating changing query and staying on the current query respectively. 
	\item The \textit{transition function} $P$ 
	\item The system goal is to extract the closest extracted organisation names and dates to the gold standards by generating less number of queries. For this reason, choosing $q$ action in any sate has a negative reward. And every time the extracted organisation name or date from a new snippet is close to the gold standards, the system receives a positive reward for instance $r(s, q) = -0.1$ or $r(s, d) = 10$ if the extracted organisation name (year) is in the list of organisation names (years) for the gold standards. 
\end{itemize}

